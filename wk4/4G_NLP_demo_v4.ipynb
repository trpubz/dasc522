{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "4G NLP demo v4.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKvF75oqpH8a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## This notebook is modified from a Winter 2021 DASC 522 student\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8ZA0FEG-7aJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Define a confusion matrix function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xodqcgMG--NX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# This is from https://github.com/DTrimarchi10/confusion_matrix/blob/master/cf_matrix.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sKQKO4wkw6TQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# ------ Common formating options ---------\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "SMALL_SIZE = 20\n",
    "MEDIUM_SIZE = 30\n",
    "BIGGER_SIZE = 40\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)           # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)      # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)   # fontsize of the figure title\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "# ------ end format ---------"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAnC77lf7Wpx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import Data and Basic Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxwGRBlGhTHr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3CIyr3rrUJ5",
    "outputId": "0d1644ff-a0d8-4679-94be-880b2a8c69bd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "print(\" \")\n",
    "\n",
    "df = pd.read_csv('NLP Training data.csv', engine='python', encoding_errors='ignore')\n",
    "\n",
    "print(\" \")\n",
    "df.info()\n",
    "print(\" \")\n",
    "\n",
    "print(\"---1st 15 job titles---\")\n",
    "for loop in range(0,14):\n",
    "  print(df['JobTitle'][loop])\n",
    "\n",
    "print(\" \")"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 365 entries, 0 to 364\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   User ID                365 non-null    int64 \n",
      " 1   Professional Field     65 non-null     object\n",
      " 2   Organization           243 non-null    object\n",
      " 3   Military or Civilian:  363 non-null    object\n",
      " 4   JobTitle               365 non-null    object\n",
      " 5   Country/Region Name    365 non-null    object\n",
      " 6   Likes Lattes           365 non-null    object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 20.1+ KB\n",
      " \n",
      "---1st 15 job titles---\n",
      "Operations Sergeant\n",
      "Senior Advisor\n",
      "South East Asia Branch Chief/CG LNO\n",
      "Professor\n",
      "Masters in Marine Affairs Candidate\n",
      "Intel Specialist\n",
      "SOJ7 Director\n",
      "Fellow\n",
      "Supervisory Special Agent\n",
      "Exercise Scenario Developer\n",
      "Instructional systems specialist\n",
      "Counter Threat Finance (CTF) Analyst\n",
      "Company Commander\n",
      "Special Agent\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/trpubz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2krVdTRHhsGQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Standardize text "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNHh_RCzhuGH",
    "outputId": "c2dc8277-10a3-4889-9996-7fa23244ae5d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# df['JobTitle']=df['JobTitle'].astype(str)\n",
    "\n",
    "\n",
    "# df['JobTitle'] = df.JobTitle.str.replace('[^a-zA-Z ]', ' ')  #Removes numbers and non alphabet characters (slashes, punctuation, etc)\n",
    "df['JobTitle'] = df.JobTitle.str.replace('[^a-zA-Z0-9 ]', ' ')   #Removes numbers non alphabet characters\n",
    "df['JobTitle'] = df.JobTitle.str.lower()  #Makes all text lower case (Makes sure AFI and afi (i.e. policy indicators) are treated the same\n",
    "\n",
    "print(\"---1st 15 job titles---\")\n",
    "for loop in range(0,14):\n",
    "  print(df['JobTitle'][loop])\n",
    "\n",
    "print(\" \")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmUERABjOz2Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Print dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrjO6NTdO2cY",
    "outputId": "c367a1d4-81a2-401a-b512-c01b373a96d0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "df.info()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e8wIeONlwUe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpGBNIW9lxou",
    "outputId": "8d9cb773-92a3-475c-882b-3a66968ec3ef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "df['JobTitle'] = df.apply(lambda row: nltk.word_tokenize(row['JobTitle']), axis=1)\n",
    "#Removes full string list in \"subject\" and turns it into a list of words\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "for row in df['JobTitle']:\n",
    "  for x in row:\n",
    "    if x in stopwords.words('english'): row.remove(x)\n",
    "#removes \"stop words\" like \"for\", \"a\", \"the\", etc\n",
    "df\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "for loop in range(1,5):\n",
    "  print(df['JobTitle'][loop])\n",
    "\n",
    "print(\" \")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 926
    },
    "id": "DjZsLsqnt89A",
    "outputId": "6308a620-f886-4a31-d0d1-827211a70b3f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#table showing most common words in subject\n",
    "i = []\n",
    "for row in df['JobTitle']:\n",
    "  for x in row:\n",
    "    i.append(x)\n",
    "j = pd.DataFrame(i)\n",
    "j.value_counts()[:25].plot(kind='bar')\n",
    "\n",
    "from collections import Counter\n",
    "Counter = Counter(i)\n",
    "most_occur = Counter.most_common(50)  #350 is max unique words\n",
    "most = pd.DataFrame(most_occur).T\n",
    "most"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqRXRFxauDD4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Rejoin the job titles"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBMFu7QouEiO",
    "outputId": "f7e7bad5-59b0-4394-d073-a75c0e2c0dd3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#Create new \"joined\" column - needed for Count Vectorization in next problem\n",
    "# - Count Vectorization gets triped up over a list of strings, needs solid list\n",
    "#  This hits \"undo\" on Tokenization\n",
    "\n",
    "df['Joined'] = df.JobTitle.apply(' '.join)\n",
    "\n",
    "for loop in range(1,5):\n",
    "  print(df['Joined'][loop])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3hlt4h8oeLa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "R9TejxDXogTp",
    "outputId": "af7f05fc-eb4b-4d73-95e3-6dca19757e6c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "features_to_extract = 10\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = features_to_extract) \n",
    "\n",
    "X = vectorizer.fit_transform(df.Joined).toarray()  # new variable - input to the model\n",
    "\n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "most_occur = Counter.most_common(features_to_extract)\n",
    "most = pd.DataFrame(most_occur)\n",
    "most\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cUfIaStwoLP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Vectorize applied to data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCCwKy9rwn2f",
    "outputId": "9893c3db-3db0-4d2a-b5f0-f075580e7599",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "for loop in range(1,10):\n",
    "  print(df['Joined'][loop])\n",
    "\n",
    "print(\" \")\n",
    "print(\"------\")\n",
    "print(\" \")\n",
    "\n",
    "print(X[1:10])\n",
    "\n",
    "# 'air': 1   2nd column is if the text contains \"air\"\n",
    "# 'coordination': 2    3rd column is if the text contains \"coordination\"\n",
    "\n",
    "#  The X that results can be used in any modeling method!\n",
    "\n",
    "# Vocabulary: 1st column =Analyst, 2nd = chief, 3rd = deputy, 4th = director, 5th = officer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TUzrfBNznG1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Classical Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7FSMcdGyxmF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a model using the most common 20 words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8_-iSAsNyyp6",
    "outputId": "37ec11c5-8b0e-48e2-c156-53a254f546cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#Feature Matrix Creation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features = 2)   # 20 words\n",
    "X = vectorizer.fit_transform(df.Joined).toarray()\n",
    "\n",
    "# print(X)\n",
    "print(\"------\")\n",
    "\n",
    "# Split out y variable, make it boolean\n",
    "y = df.iloc[:, 6]\n",
    "y.astype(bool)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score (y_test, y_pred)\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy = balanced_accuracy_score (y_test, y_pred)\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(f\"Balanced Accuracy = {balanced_accuracy}\")\n",
    "\n",
    "# The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. \n",
    "# It is defined as the average of recall obtained on each class.\n",
    "\n",
    "print(\"------\")\n",
    "\n",
    "############# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['Likes Lattes', 'Nope']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "############## Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "make_confusion_matrix(cf_matrix, \n",
    "                      categories=target_names)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB7lTRZJyxkB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Model sweep over number of most-common input words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "s75wG9EYuULp",
    "outputId": "000eed56-b574-4933-ef18-62d2ead12e9c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#This code section performs a sweep and plots out final accuracy based on used words\n",
    "#Code uses a Naive Bayes classification prediction algorithm from sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "max_words = 200\n",
    "\n",
    "accuracy_to_save = []\n",
    "balanced_accuracy_to_save = []\n",
    "\n",
    "for max_f in range(1,max_words):\n",
    "\n",
    "  #Feature Matrix Creation\n",
    "  from sklearn.feature_extraction.text import CountVectorizer\n",
    "  vectorizer = CountVectorizer(max_features = max_f)  \n",
    "  X = vectorizer.fit_transform(df.Joined).toarray()\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "  #Naive Bayes\n",
    "  from sklearn.naive_bayes import GaussianNB\n",
    "  classifier = GaussianNB()\n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "  #Predict\n",
    "  y_pred = classifier.predict(X_test)\n",
    "\n",
    "  # Accuracy\n",
    "  from sklearn.metrics import accuracy_score\n",
    "  accuracy = accuracy_score (y_test, y_pred)\n",
    "  from sklearn.metrics import balanced_accuracy_score\n",
    "  balanced_accuracy = balanced_accuracy_score (y_test, y_pred)\n",
    "#  print(accuracy)\n",
    "#  print(balanced_accuracy)\n",
    "\n",
    "  balanced_accuracy_to_save.append(balanced_accuracy)\n",
    "  accuracy_to_save.append(accuracy)\n",
    "\n",
    "#plt.plot(range(1,max_words), balanced_accuracy_to_save, label=\"balanced_accuracy\")\n",
    "\n",
    "plotting=1\n",
    "if plotting:\n",
    "  fig = plt.plot(range(1,max_words), accuracy_to_save, label=\"accuracy\")\n",
    "  plt.xlabel('Words Used')\n",
    "  plt.ylabel('Accuracy')\n",
    "  fig\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPwQWoVu7ddU",
    "outputId": "a92a2f84-f37f-4820-9aff-6f332272b977",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "for loop in range(1,150):\n",
    "  print(f\"words={loop} accuracy={accuracy_to_save[loop-1]} \")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmbNhRMn06hO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Neural network\n",
    "single vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "D9X9VcKe0-D1",
    "outputId": "2b152935-9f65-4314-8c3d-90cb7043be84",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow\n",
    "from keras.layers import Flatten\n",
    "\n",
    "MAX_NB_WORDS = 50  # The maximum number of words to be used (most frequent) aka vocabulary size\n",
    "\n",
    "# error prevention\n",
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "# create vocab from max words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features = MAX_NB_WORDS)  \n",
    "X = vectorizer.fit_transform(df.Joined).toarray()\n",
    "\n",
    "# Split out y variable, make it 0 and 1\n",
    "y = df.iloc[:, 6]\n",
    "y = (y=='YES')*1\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(5, input_dim=MAX_NB_WORDS, activation='relu'))\n",
    "\n",
    "#model.add(Dense(50, activation='relu'))\n",
    "#model.add(Dense(40, activation='relu'))\n",
    "#model.add(Dense(20, activation='relu'))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.15)\n",
    "\n",
    "\n",
    "\n",
    "# evaluate the keras model on test set\n",
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show();\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX0crnAv6Ozp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sweep of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mN1TtXvG6OGn",
    "outputId": "7bd5df7c-d0c6-42a4-c0ab-be9bfd0ca65c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "max_words = 100 # 350 is maximum unique words in this dataset's vocab\n",
    "\n",
    "accuracy_test_to_save = []\n",
    "accuracy_train_to_save = []\n",
    "\n",
    "for max_f in range(1,max_words):  \n",
    "\n",
    "  # error prevention\n",
    "  from keras import backend as K \n",
    "  K.clear_session()\n",
    "\n",
    "  # create vocab from max words\n",
    "  from sklearn.feature_extraction.text import CountVectorizer\n",
    "  vectorizer = CountVectorizer(max_features = max_f)  \n",
    "  X = vectorizer.fit_transform(df.Joined).toarray()\n",
    "\n",
    "  # Split out y variable, make it 0 and 1\n",
    "  y = df.iloc[:, 6]\n",
    "  y = (y=='YES')*1\n",
    "\n",
    "  # Split data\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Dense(5, input_dim=max_f, activation='relu'))\n",
    "\n",
    "  #model.add(Dense(50, activation='relu'))\n",
    "  #model.add(Dense(40, activation='relu'))\n",
    "  #model.add(Dense(20, activation='relu'))\n",
    "  #model.add(Dense(10, activation='relu'))\n",
    "\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "  epochs = 40\n",
    "  batch_size = 32\n",
    "\n",
    "  history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.15, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "  # evaluate the keras model on test set\n",
    "  acc_train = model.evaluate(X_train,y_train)\n",
    "  acc_test = model.evaluate(X_test,y_test)\n",
    "  print(f'{max_f} words: Train/Test set Accuracy: {acc_train[1]}/{acc_test[1]}')\n",
    "\n",
    "  accuracy_train_to_save.append(acc_train[1])\n",
    "  accuracy_test_to_save.append(acc_test[1])\n",
    "\n",
    "\n",
    "fig = plt.scatter(range(1,max_words), accuracy_train_to_save, label=\"train accuracy\")\n",
    "fig = plt.scatter(range(1,max_words), accuracy_test_to_save, label=\"holdout accuracy\")\n",
    "\n",
    "plt.xlabel('Words Used')\n",
    "plt.ylabel('Accuracy ')\n",
    "# plt.ylim(0.3, 0.75)\n",
    "plt.legend()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxgtnw4HBP02",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "38 words seems best, evaluate metrics more thoroughly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_f = 38  # The maximum number of words to be used (most frequent) aka vocabulary size\n",
    "\n",
    "# error prevention\n",
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "# create vocab from max words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features = max_f)  \n",
    "X = vectorizer.fit_transform(df.Joined).toarray()\n",
    "\n",
    "# Split out y variable, make it 0 and 1\n",
    "y = df.iloc[:, 6]\n",
    "y = (y=='YES')*1\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(5, input_dim=max_f, activation='relu'))\n",
    "\n",
    "model.add(Dense(5, activation='relu'))\n",
    "# model.add(Dense(40, activation='relu'))\n",
    "# model.add(Dense(20, activation='relu'))\n",
    "# model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.15)\n",
    "\n",
    "\n",
    "\n",
    "# evaluate the keras model on test set\n",
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "# evaluate the keras model on test set\n",
    "acc_train = model.evaluate(X_train,y_train)\n",
    "acc_test = model.evaluate(X_test,y_test)\n",
    "print(f'{max_f} words: Train/holdout set Accuracy: {acc_train[1]}/{acc_test[1]}')\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Thorough metric evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "#Predict\n",
    "y_pred_test = np.round(model.predict(X_test))\n",
    "y_pred_train = np.round(model.predict(X_train))\n",
    "y_pred_all = np.round(model.predict(X))\n",
    "\n",
    "# Metrics for Test\n",
    "print(\"---------- Metrics for test dataset -------\")\n",
    "\n",
    "print(f\"Accuracy test =   { accuracy_score (y_test, y_pred_test) }\")\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "make_confusion_matrix(cf_matrix, \n",
    "                      categories=target_names)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "\n",
    "print(\"---------- Metrics for train dataset -------\")\n",
    "\n",
    "print(f\"Accuracy train = { accuracy_score (y_train, y_pred_train) }\")\n",
    "\n",
    "cf_matrix = confusion_matrix(y_train, y_pred_train)\n",
    "make_confusion_matrix(cf_matrix, \n",
    "                      categories=target_names)\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "\n",
    "print(\"---------- Metrics for all -------\")\n",
    "\n",
    "print(f\"Accuracy train = { accuracy_score (y, y_pred_all) }\")\n",
    "\n",
    "cf_matrix = confusion_matrix(y, y_pred_all)\n",
    "make_confusion_matrix(cf_matrix, \n",
    "                      categories=target_names)\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(y, y_pred_all))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}